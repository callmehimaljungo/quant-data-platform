# ğŸ”„ SO SÃNH CODE: PARQUET â†’ DELTA LAKE

## TÃ“M Táº®T

**Thay Ä‘á»•i tá»‘i thiá»ƒu, hiá»‡u quáº£ tá»‘i Ä‘a!**

```
Parquet (hiá»‡n táº¡i)        â†’    Delta Lake (Lakehouse)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Giá»¯ nguyÃªn 95% code           âœ… Chá»‰ sá»­a read/write
âœ… Logic khÃ´ng Ä‘á»•i                âœ… ThÃªm ACID, Time Travel
âœ… Schema validation giá»¯          âœ… Transaction log miá»…n phÃ­
```

---

## ğŸ“¦ DEPENDENCIES

### Hiá»‡n Táº¡i (Parquet)
```txt
# requirements.txt - HIá»†N Táº I
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=14.0.0  # Cho Parquet
kaggle>=1.5.0
```

### Sau Migration (Delta Lake)
```txt
# requirements.txt - SAU KHI MIGRATE
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=14.0.0       # Váº«n cáº§n (Delta dÃ¹ng Parquet)
delta-spark>=2.4.0    # THÃŠM Má»šI
pyspark>=3.4.0        # THÃŠM Má»šI
kaggle>=1.5.0
```

**Thay Ä‘á»•i:** Chá»‰ thÃªm 2 packages!

---

## ğŸ”§ HELPER FUNCTIONS (Táº¡o 1 láº§n, dÃ¹ng mÃ£i)

```python
# utils/delta_helper.py - FILE Má»šI

"""
Delta Lake Helper Functions
Táº¡o 1 láº§n, dÃ¹ng cho táº¥t cáº£ layers
"""

from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession
import pandas as pd

def get_spark_session(app_name="QuanPlatform"):
    """
    Initialize Spark with Delta Lake support
    
    Chá»‰ cáº§n gá»i 1 láº§n khi báº¯t Ä‘áº§u script
    """
    builder = SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.driver.memory", "4g")  # TÃ¹y chá»‰nh theo RAM
    
    return configure_spark_with_delta_pip(builder).getOrCreate()

def pandas_to_delta(df_pandas, path, mode="overwrite"):
    """
    Save pandas DataFrame as Delta Table
    
    Args:
        df_pandas: pandas DataFrame
        path: Output path (e.g., './data/bronze/prices_delta')
        mode: 'overwrite' or 'append'
    """
    spark = get_spark_session()
    spark_df = spark.createDataFrame(df_pandas)
    
    spark_df.write.format("delta") \
        .mode(mode) \
        .option("overwriteSchema", "true") \
        .save(path)
    
    return path

def delta_to_pandas(path, version=None, timestamp=None):
    """
    Read Delta Table to pandas DataFrame
    
    Args:
        path: Delta Table path
        version: Optional version number (for Time Travel)
        timestamp: Optional timestamp (for Time Travel)
    
    Returns:
        pandas DataFrame
    """
    spark = get_spark_session()
    
    reader = spark.read.format("delta")
    
    # Time Travel support
    if version is not None:
        reader = reader.option("versionAsOf", version)
    elif timestamp is not None:
        reader = reader.option("timestampAsOf", timestamp)
    
    spark_df = reader.load(path)
    return spark_df.toPandas()
```

---

## ğŸ“„ BRONZE LAYER

### BEFORE (Parquet)

```python
# bronze/ingest.py - HIá»†N Táº I

import pandas as pd
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

def ingest_from_kaggle():
    """Download from Kaggle"""
    # ... logic download ... (GIá»® NGUYÃŠN)
    df = pd.read_csv('./temp/all_stock_data.csv')
    return df

def save_to_bronze(df):
    """Save to Bronze layer"""
    output_path = './data/bronze/prices.parquet'
    
    # Add metadata
    df['ingested_at'] = datetime.now()
    
    # Save as Parquet
    df.to_parquet(output_path, index=False)
    
    logger.info(f"Saved {len(df)} rows to {output_path}")

def main():
    df = ingest_from_kaggle()
    save_to_bronze(df)

if __name__ == "__main__":
    main()
```

### AFTER (Delta Lake)

```python
# bronze/ingest.py - SAU KHI MIGRATE

import pandas as pd
import logging
from datetime import datetime
from utils.delta_helper import pandas_to_delta  # THÃŠM Má»šI

logger = logging.getLogger(__name__)

def ingest_from_kaggle():
    """Download from Kaggle"""
    # ... logic download ... (GIá»® NGUYÃŠN - KHÃ”NG Äá»”I)
    df = pd.read_csv('./temp/all_stock_data.csv')
    return df

def save_to_bronze(df):
    """Save to Bronze layer as Delta Table"""
    output_path = './data/bronze/prices_delta'  # Äá»•i path
    
    # Add metadata (GIá»® NGUYÃŠN)
    df['ingested_at'] = datetime.now()
    
    # Save as Delta Table (THAY 1 DÃ’NG)
    pandas_to_delta(df, output_path, mode="overwrite")
    
    logger.info(f"Saved {len(df)} rows to Delta Table: {output_path}")
    
    # THÃŠM: Log transaction history
    from delta.tables import DeltaTable
    spark = get_spark_session()
    deltaTable = DeltaTable.forPath(spark, output_path)
    logger.info("Latest transactions:")
    deltaTable.history().show(5)

def main():
    df = ingest_from_kaggle()  # GIá»® NGUYÃŠN
    save_to_bronze(df)         # GIá»® NGUYÃŠN

if __name__ == "__main__":
    main()
```

**THAY Äá»”I:**
- âœï¸ Import helper function
- âœï¸ Äá»•i path (thÃªm `_delta`)
- âœï¸ Thay `.to_parquet()` â†’ `pandas_to_delta()`
- â• ThÃªm log transaction history (optional)

**GIá»® NGUYÃŠN:**
- âœ… Kaggle download logic
- âœ… Schema validation
- âœ… Metadata columns
- âœ… Logging

---

## ğŸ“„ SILVER LAYER

### BEFORE (Parquet)

```python
# silver/clean.py - HIá»†N Táº I

import pandas as pd
import logging

logger = logging.getLogger(__name__)

def load_bronze():
    """Load from Bronze"""
    return pd.read_parquet('./data/bronze/prices.parquet')

def clean_data(df):
    """Clean and enrich data"""
    # Deduplicate (GIá»® NGUYÃŠN)
    df = df.drop_duplicates(subset=['ticker', 'date'])
    
    # Quality gates (GIá»® NGUYÃŠN)
    if (df['close'] <= 0).any():
        raise ValueError("Invalid prices detected!")
    
    # Calculate daily return (GIá»® NGUYÃŠN)
    df = df.sort_values(['ticker', 'date'])
    df['daily_return'] = df.groupby('ticker')['close'].pct_change()
    
    return df

def save_to_silver(df):
    """Save to Silver layer"""
    output_path = './data/silver/enriched_stocks.parquet'
    df.to_parquet(output_path, index=False)
    logger.info(f"Saved to {output_path}")

def main():
    df = load_bronze()
    df_clean = clean_data(df)
    save_to_silver(df_clean)

if __name__ == "__main__":
    main()
```

### AFTER (Delta Lake)

```python
# silver/clean.py - SAU KHI MIGRATE

import pandas as pd
import logging
from utils.delta_helper import delta_to_pandas, pandas_to_delta  # THÃŠM

logger = logging.getLogger(__name__)

def load_bronze():
    """Load from Bronze Delta Table"""
    return delta_to_pandas('./data/bronze/prices_delta')  # Äá»”I 1 DÃ’NG

def clean_data(df):
    """Clean and enrich data"""
    # Deduplicate (GIá»® NGUYÃŠN - KHÃ”NG Äá»”I)
    df = df.drop_duplicates(subset=['ticker', 'date'])
    
    # Quality gates (GIá»® NGUYÃŠN - KHÃ”NG Äá»”I)
    if (df['close'] <= 0).any():
        raise ValueError("Invalid prices detected!")
    
    # Calculate daily return (GIá»® NGUYÃŠN - KHÃ”NG Äá»”I)
    df = df.sort_values(['ticker', 'date'])
    df['daily_return'] = df.groupby('ticker')['close'].pct_change()
    
    return df

def save_to_silver(df):
    """Save to Silver layer as Delta Table"""
    output_path = './data/silver/enriched_stocks_delta'  # Äá»•i path
    pandas_to_delta(df, output_path, mode="overwrite")  # Äá»”I 1 DÃ’NG
    logger.info(f"Saved to Delta Table: {output_path}")

def main():
    df = load_bronze()          # GIá»® NGUYÃŠN
    df_clean = clean_data(df)   # GIá»® NGUYÃŠN
    save_to_silver(df_clean)    # GIá»® NGUYÃŠN

if __name__ == "__main__":
    main()
```

**THAY Äá»”I:**
- âœï¸ `pd.read_parquet()` â†’ `delta_to_pandas()`
- âœï¸ `df.to_parquet()` â†’ `pandas_to_delta()`
- âœï¸ Äá»•i path

**GIá»® NGUYÃŠN:**
- âœ… Deduplication logic
- âœ… Quality gates
- âœ… Daily return calculation
- âœ… Táº¥t cáº£ business logic

---

## ğŸ“„ GOLD LAYER (Sector Analysis Example)

### BEFORE (Parquet)

```python
# gold/sector_analysis.py - HIá»†N Táº I

import pandas as pd
import numpy as np

def load_silver():
    return pd.read_parquet('./data/silver/enriched_stocks.parquet')

def calculate_sector_performance(df):
    """Calculate sector metrics"""
    # Sector average return (GIá»® NGUYÃŠN)
    sector_perf = df.groupby(['date', 'sector'])['daily_return'].mean()
    
    # Sector volatility (GIá»® NGUYÃŠN)
    sector_vol = df.groupby('sector')['daily_return'].std() * np.sqrt(252)
    
    return sector_perf, sector_vol

def save_results(sector_perf, sector_vol):
    sector_perf.to_parquet('./data/gold/sector_performance.parquet')
    sector_vol.to_parquet('./data/gold/sector_volatility.parquet')

def main():
    df = load_silver()
    perf, vol = calculate_sector_performance(df)
    save_results(perf, vol)

if __name__ == "__main__":
    main()
```

### AFTER (Delta Lake)

```python
# gold/sector_analysis.py - SAU KHI MIGRATE

import pandas as pd
import numpy as np
from utils.delta_helper import delta_to_pandas, pandas_to_delta  # THÃŠM

def load_silver():
    return delta_to_pandas('./data/silver/enriched_stocks_delta')  # Äá»”I

def calculate_sector_performance(df):
    """Calculate sector metrics"""
    # Sector average return (GIá»® NGUYÃŠN - KHÃ”NG Äá»”I)
    sector_perf = df.groupby(['date', 'sector'])['daily_return'].mean()
    
    # Sector volatility (GIá»® NGUYÃŠN - KHÃ”NG Äá»”I)
    sector_vol = df.groupby('sector')['daily_return'].std() * np.sqrt(252)
    
    return sector_perf, sector_vol

def save_results(sector_perf, sector_vol):
    # Convert Series to DataFrame for Delta
    perf_df = sector_perf.reset_index()
    vol_df = sector_vol.reset_index()
    
    pandas_to_delta(perf_df, './data/gold/sector_performance_delta')  # Äá»”I
    pandas_to_delta(vol_df, './data/gold/sector_volatility_delta')    # Äá»”I

def main():
    df = load_silver()                          # GIá»® NGUYÃŠN
    perf, vol = calculate_sector_performance(df)  # GIá»® NGUYÃŠN
    save_results(perf, vol)                     # GIá»® NGUYÃŠN

if __name__ == "__main__":
    main()
```

**THAY Äá»”I:**
- âœï¸ Read/Write functions
- âœï¸ Paths

**GIá»® NGUYÃŠN:**
- âœ… All calculation logic
- âœ… Business formulas
- âœ… GroupBy operations

---

## ğŸ BONUS: TIME TRAVEL (TIER 2 Feature)

### Code Má»›i ThÃªm (KhÃ´ng Sá»­a Code CÅ©)

```python
# utils/time_travel.py - FILE Má»šI

from utils.delta_helper import delta_to_pandas, get_spark_session
from delta.tables import DeltaTable

def compare_versions(table_path, version1, version2):
    """
    So sÃ¡nh 2 versions cá»§a Delta Table
    
    Use case: Xem data thay Ä‘á»•i nhÆ° tháº¿ nÃ o sau má»—i láº§n update
    """
    df_v1 = delta_to_pandas(table_path, version=version1)
    df_v2 = delta_to_pandas(table_path, version=version2)
    
    # Find differences
    new_rows = len(df_v2) - len(df_v1)
    print(f"Version {version1} â†’ {version2}:")
    print(f"  Rows changed: {new_rows:+d}")
    
    return df_v1, df_v2

def rollback_to_version(table_path, target_version):
    """
    Rollback Delta Table vá» version cÅ©
    
    Use case: PhÃ¡t hiá»‡n lá»—i, cáº§n quay láº¡i version trÆ°á»›c
    """
    spark = get_spark_session()
    deltaTable = DeltaTable.forPath(spark, table_path)
    
    # Restore to target version
    deltaTable.restoreToVersion(target_version)
    
    print(f"âœ“ Rolled back to version {target_version}")

def show_history(table_path, num_versions=10):
    """
    Xem lá»‹ch sá»­ thay Ä‘á»•i cá»§a Delta Table
    
    Use case: Audit trail, biáº¿t ai lÃ m gÃ¬ khi nÃ o
    """
    spark = get_spark_session()
    deltaTable = DeltaTable.forPath(spark, table_path)
    
    history = deltaTable.history(num_versions)
    history.select("version", "timestamp", "operation", "operationMetrics").show()
    
    return history

# DEMO Usage
if __name__ == "__main__":
    table_path = './data/silver/enriched_stocks_delta'
    
    # 1. Xem lá»‹ch sá»­
    print("=== Transaction History ===")
    show_history(table_path)
    
    # 2. So sÃ¡nh versions
    print("\n=== Compare Versions ===")
    df_v1, df_v2 = compare_versions(table_path, version1=0, version2=1)
    
    # 3. Rollback (náº¿u cáº§n)
    # rollback_to_version(table_path, target_version=0)
```

---

## ğŸ“Š MIGRATION CHECKLIST

### BÆ°á»›c 1: Chuáº©n Bá»‹ (30 phÃºt)

```bash
# CÃ i packages
pip install delta-spark>=2.4.0 pyspark>=3.4.0

# Táº¡o helper file
# Copy code tá»« pháº§n "HELPER FUNCTIONS" á»Ÿ trÃªn
mkdir utils
# Táº¡o utils/delta_helper.py
```

### BÆ°á»›c 2: Test Vá»›i Sample (1 giá»)

```python
# test_delta.py - File test

from utils.delta_helper import pandas_to_delta, delta_to_pandas
import pandas as pd

# Test vá»›i data nhá»
df_test = pd.DataFrame({
    'ticker': ['AAPL', 'MSFT'],
    'close': [150.0, 250.0]
})

# Test write
pandas_to_delta(df_test, './test_delta')
print("âœ“ Write OK")

# Test read
df_read = delta_to_pandas('./test_delta')
print("âœ“ Read OK")
print(df_read)
```

### BÆ°á»›c 3: Migrate Bronze (1 giá»)

```bash
# Backup code cÅ©
cp bronze/ingest.py bronze/ingest_parquet_backup.py

# Sá»­a bronze/ingest.py theo template "AFTER" á»Ÿ trÃªn

# Test
python bronze/ingest.py
```

### BÆ°á»›c 4: Migrate Silver (1 giá»)

```bash
# Backup
cp silver/clean.py silver/clean_parquet_backup.py

# Sá»­a silver/clean.py

# Test
python silver/clean.py
```

### BÆ°á»›c 5: Migrate Gold (1-2 giá»)

```bash
# Sá»­a táº¥t cáº£ files trong gold/

# Test tá»«ng file
python gold/sector_analysis.py
python gold/risk_metrics.py
python gold/portfolio.py
```

### BÆ°á»›c 6: End-to-End Test (30 phÃºt)

```bash
# Cháº¡y full pipeline
python bronze/ingest.py
python silver/clean.py
python gold/sector_analysis.py

# Verify outputs
ls -lh data/bronze/
ls -lh data/silver/
ls -lh data/gold/
```

---

## ğŸ’¡ TIPS & TRICKS

### Tip 1: Hybrid Approach (Giá»¯ Cáº£ Hai)

Náº¿u lo láº¯ng, báº¡n cÃ³ thá»ƒ giá»¯ cáº£ Parquet vÃ  Delta:

```python
def save_to_bronze(df):
    """Save to both Parquet and Delta"""
    # Parquet (backup)
    df.to_parquet('./data/bronze/prices.parquet')
    
    # Delta (main)
    pandas_to_delta(df, './data/bronze/prices_delta')
    
    # Best of both worlds!
```

### Tip 2: Conditional Import

```python
# bronze/ingest.py - Smart import

try:
    from utils.delta_helper import pandas_to_delta
    USE_DELTA = True
except ImportError:
    USE_DELTA = False
    print("Delta Lake not available, using Parquet")

def save_to_bronze(df):
    if USE_DELTA:
        pandas_to_delta(df, './data/bronze/prices_delta')
    else:
        df.to_parquet('./data/bronze/prices.parquet')
```

### Tip 3: Gradual Migration

```
Week 1: Chá»‰ migrate Bronze â†’ Test ká»¹
Week 2: Migrate Silver â†’ Test ká»¹
Week 3: Migrate Gold â†’ Test ká»¹

â†’ Tá»« tá»«, cháº¯c cháº¯n hÆ¡n build láº¡i tá»« Ä‘áº§u!
```

---

## ğŸ¯ TÃ“M Táº®T SO SÃNH

### Effort Required

| Task | Thá»i Gian | Äá»™ KhÃ³ |
|------|-----------|--------|
| **Setup Delta** | 30 phÃºt | â­ Dá»… |
| **Create Helper Functions** | 1 giá» | â­â­ Trung bÃ¬nh |
| **Migrate Bronze** | 1 giá» | â­ Dá»… |
| **Migrate Silver** | 1 giá» | â­ Dá»… |
| **Migrate Gold** | 2 giá» | â­â­ Trung bÃ¬nh |
| **Testing** | 2 giá» | â­ Dá»… |
| **TOTAL** | **~7-8 giá»** | **â­â­ Trung bÃ¬nh** |

### Code Changes

```
Total Files:      ~8 files
Changed Lines:    ~50 lines
New Lines:        ~100 lines (helper functions)
Business Logic:   0 changes! âœ…

â†’ 95% code GIá»® NGUYÃŠN!
```

### Benefits

```
PARQUET (Hiá»‡n táº¡i)          DELTA LAKE (Sau migrate)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ No ACID                  âœ… ACID transactions
âŒ No versioning            âœ… Time Travel
âŒ No metadata              âœ… Transaction log
âŒ Overwrite only           âœ… Upsert/Merge
âŒ No rollback              âœ… Rollback to any version
âš ï¸ Manual validation        âœ… Built-in constraints
âš ï¸ Risk of data loss        âœ… Safe concurrent writes

â†’ Upgrade Xá»¨NG ÄÃNG vá»›i 7-8 giá» effort!
```

---

## âœ… Káº¾T LUáº¬N

**Migration tá»« Parquet â†’ Delta Lake:**
- â° Thá»i gian: 1-2 ngÃ y
- ğŸ’ª Äá»™ khÃ³: Trung bÃ¬nh
- ğŸ“ Code changes: Tá»‘i thiá»ƒu
- ğŸ Benefits: Ráº¥t lá»›n
- âš ï¸ Risk: Tháº¥p (cÃ³ thá»ƒ giá»¯ Parquet backup)

**â†’ TOTALLY WORTH IT cho Ä‘á»“ Ã¡n Lakehouse!**

---

**Ready to migrate? Let's do it! ğŸš€**
